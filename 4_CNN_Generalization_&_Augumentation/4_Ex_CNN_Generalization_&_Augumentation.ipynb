{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.16.1\n",
      "Keras version: 3.2.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os, shutil\n",
    "\n",
    "print (\"TensorFlow version: \" + tf.__version__)\n",
    "print (\"Keras version: \" + keras.__version__)\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_confusion_matrix_for_models(X_test, Y_test, *models) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes the confusion matrices for multiple machine learning models.\n",
    "\n",
    "    Parameters:\n",
    "        X_test (array-like): Test data features.\n",
    "        Y_test (array-like): True labels for the test data.\n",
    "        *models (variable-length argument list): Machine learning models to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays confusion matrices using seaborn's heatmap.\n",
    "    \"\"\"\n",
    "    num_models = len(models)\n",
    "    plt.figure(figsize=(5 * num_models, 5))\n",
    "    for i, model in enumerate(models, 1):\n",
    "        plt.subplot(1, num_models, i)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        Y_pred = (Y_pred > 0.5)\n",
    "        matrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
    "        hm = sns.heatmap(matrix,\n",
    "                         cmap='coolwarm',\n",
    "                         linecolor='white',\n",
    "                         linewidths=1,\n",
    "                         annot=True,\n",
    "                         fmt='d')\n",
    "        plt.yticks(rotation=0)\n",
    "        hm.set_ylim(0, len(matrix))\n",
    "        plt.title(f'Confusion Matrix - Model {i}')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_accuracy_comparison(*histories, labels: list[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plots the training and validation accuracy comparison for multiple models.\n",
    "\n",
    "    Args:\n",
    "    *histories: Variable number of history objects. Each history object should contain\n",
    "                training and validation accuracy values.\n",
    "    labels: List of labels for each model.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (25.0, 5.0)  # set default size of plots\n",
    "    epochs = range(1, len(histories[0].history['acc']) + 1)  # Assuming all models have the same number of epochs\n",
    "\n",
    "    for i, history in enumerate(histories):\n",
    "        color = ['b', 'r', 'g', 'c', 'm', 'y'][i % 6]  # Choose color cyclically\n",
    "        label = labels[i] if labels else f'Model {i+1}'\n",
    "        acc = history.history['acc']\n",
    "        val_acc = history.history['val_acc']\n",
    "        plt.plot(epochs, acc, color + 'o', label=f'Training accuracy for {label}')\n",
    "        plt.plot(epochs, val_acc, color, label=f'Validation accuracy for {label}')\n",
    "\n",
    "    plt.title('Comparison of Training and Validation Accuracies')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_loss_comparison(*histories, labels: list[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Plots the training and validation loss comparison for multiple models.\n",
    "\n",
    "    Args:\n",
    "    *histories: Variable number of history objects. Each history object should contain\n",
    "                training and validation loss values.\n",
    "    labels: List of labels for each model.\n",
    "    \"\"\"\n",
    "    plt.rcParams['figure.figsize'] = (25.0, 5.0)  # set default size of plots\n",
    "    epochs = range(1, len(histories[0].history['loss']) + 1)  # Assuming all models have the same number of epochs\n",
    "\n",
    "    for i, history in enumerate(histories):\n",
    "        color = ['b', 'r', 'g', 'c', 'm', 'y'][i % 6]  # Choose color cyclically\n",
    "        label = labels[i] if labels else f'Model {i+1}'\n",
    "        loss = history.history['loss']\n",
    "        val_loss = history.history['val_loss']\n",
    "        plt.plot(epochs, loss, color + 'o', label=f'Training loss for {label}')\n",
    "        plt.plot(epochs, val_loss, color, label=f'Validation loss for {label}')\n",
    "\n",
    "    plt.title('Comparison of Training and Validation Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def print_val_accuracies(history1, history2, history3):\n",
    "    val_accuracy1 = history1.history['val_acc']\n",
    "    val_accuracy2 = history2.history['val_acc']\n",
    "    val_accuracy3 = history3.history['val_acc']\n",
    "\n",
    "    argmax1 = np.argmax(val_accuracy1)\n",
    "    argmax2 = np.argmax(val_accuracy2)\n",
    "    argmax3 = np.argmax(val_accuracy3)\n",
    "\n",
    "    print ('Max accuracy for model 1 is: ', val_accuracy1[argmax1], ', achieved in the ', argmax1 , 'epoch.')\n",
    "    print ('Max accuracy for model 2 is: ', val_accuracy2[argmax2], ', achieved in the ', argmax2 , 'epoch.')\n",
    "    print ('Max accuracy for model 3 is: ', val_accuracy3[argmax3], ', achieved in the ', argmax3 , 'epoch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/ml/data/kaggle/processed/cats_and_dogs_small'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m base_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/ml/data/kaggle/processed/cats_and_dogs_small\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(base_dir):\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Directories for our training, validation and test splits\u001b[39;00m\n\u001b[1;32m     15\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/ml/data/kaggle/processed/cats_and_dogs_small'"
     ]
    }
   ],
   "source": [
    "# The path to store trained models\n",
    "models_dir = './models/'\n",
    "if not os.path.exists(models_dir):\n",
    "    os.mkdir(models_dir)\n",
    "\n",
    "# The path to the directory where the original dataset was uncompressed\n",
    "original_dataset_dir = 'C:/ml/data/kaggle/original/train'\n",
    "\n",
    "# The directory where we will store our smaller dataset\n",
    "base_dir = './kaggle/processed/cats_and_dogs_small'\n",
    "if not os.path.exists(base_dir):\n",
    "    os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training, validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "if not os.path.exists(train_dir):\n",
    "    os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "if not os.path.exists(validation_dir):\n",
    "    os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "if not os.path.exists(test_dir):\n",
    "    os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "if not os.path.exists(train_cats_dir):\n",
    "    os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "if not os.path.exists(train_dogs_dir):\n",
    "    os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "if not os.path.exists(validation_cats_dir):\n",
    "    os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "if not os.path.exists(validation_dogs_dir):\n",
    "    os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "if not os.path.exists(test_cats_dir):\n",
    "    os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "if not os.path.exists(test_dogs_dir):\n",
    "    os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating more data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 150\n",
    "img_cols = 150\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "      rotation_range=50,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      shear_range=0.2,\n",
    "      zoom_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is module with image preprocessing utilities\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "fnames = [os.path.join(train_cats_dir, fname) for fname in os.listdir(train_cats_dir)]\n",
    "\n",
    "# We pick one image to \"augment\"\n",
    "img_path = fnames[3]\n",
    "\n",
    "# Read the image and resize it\n",
    "img = image.load_img(img_path, target_size=(img_rows, img_cols))\n",
    "\n",
    "# Convert it to a Numpy array with shape (150, 150, 3)\n",
    "x = image.img_to_array(img)\n",
    "\n",
    "# Reshape it to (1, 150, 150, 3)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "# The .flow() command below generates batches of randomly transformed images.\n",
    "# It will loop indefinitely, so we need to `break` the loop at some point!\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1):\n",
    "    plt.figure(i)\n",
    "    imgplot = plt.imshow(image.array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i % 4 == 0:\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "\n",
    "model2 = models.Sequential()\n",
    "model2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_rows, img_cols, 3)))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model2.add(layers.MaxPooling2D((2, 2)))\n",
    "model2.add(layers.Flatten())\n",
    "model2.add(layers.Dropout(0.5))\n",
    "model2.add(layers.Dense(256, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=50,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,)\n",
    "\n",
    "# Note that the validation data should not be augmented!\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        # This is the target directory\n",
    "        train_dir,\n",
    "        # All images will be resized to 150x150\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=20, #32\n",
    "        # Since we use binary_crossentropy loss, we need binary labels\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=20, #32\n",
    "        class_mode='binary')\n",
    "\n",
    "#history2 = model2.fit_generator(\n",
    "history2 = model2.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=100,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save(models_dir + 'cats_and_dogs_small_2.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc2 = history2.history['acc']\n",
    "val_acc2 = history2.history['val_acc']\n",
    "loss2 = history2.history['loss']\n",
    "val_loss2 = history2.history['val_loss']\n",
    "\n",
    "plot_accuracy_comparison(history2, labels=['Model 2'])\n",
    "plot_loss_comparison(history2, labels=['Model 2'])\n",
    "print_val_accuracies(history2, history2, history2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
