{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linux users:\n",
    "# !wget -O birds.tgz https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1\n",
    "# !tar zxvf birds.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=64\n",
    "BATCH_SIZE=64\n",
    "LATENT_DIM=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import cv2\n",
    "# from tqdm import tqdm\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations, bboxes, dataset_path, img_size):\n",
    "        self.annotations = annotations\n",
    "        self.bboxes = bboxes\n",
    "        self.dataset_path = dataset_path\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.annotations.iloc[idx]['id']\n",
    "        image_path = self.annotations.iloc[idx]['path']\n",
    "        bbox = self.bboxes[self.bboxes['id'] == image_id].iloc[0]\n",
    "\n",
    "        img = cv2.imread(self.dataset_path + \"images/\" + image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img[bbox['y']:bbox['y'] + bbox['height'], bbox['x']:bbox['x'] + bbox['width'], :]\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        img = torch.tensor(img, dtype=torch.float32) / 127.5 - 1\n",
    "        \n",
    "        img = img.permute(2, 0, 1)\n",
    "\n",
    "        return img\n",
    "\n",
    "dataset_path = 'kaggle/CUB_200_2011/'\n",
    "\n",
    "# Read txt files containing info about the bounding boxes and the file paths\n",
    "bboxes = pd.read_csv(dataset_path + 'bounding_boxes.txt', sep=\" \", names=[\"id\", \"x\", \"y\", \"width\", \"height\"]).astype(int)\n",
    "annotations = pd.read_csv(dataset_path + 'images.txt', sep=\" \", names=[\"id\", \"path\"])\n",
    "\n",
    "# Create the dataset\n",
    "custom_dataset = CustomDataset(annotations, bboxes, dataset_path, IMG_SIZE)\n",
    "\n",
    "# Define the number of samples for training and testing\n",
    "limit_training = len(custom_dataset) - 300\n",
    "test = 300\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [limit_training, test])\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(train_loader, test_loader)#store in array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling / Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, filters: int, size: int, strides: int = 2, padding: str ='same', use_bn: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.convolution = nn.Conv2d(in_channels, filters, kernel_size=size, stride=strides, padding=1)\n",
    "        self.use_bn = use_bn\n",
    "        self.batchNorm = nn.BatchNorm2d(filters)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tensor:\n",
    "        x = self.convolution(x)\n",
    "        if self.use_bn:\n",
    "            x = self.batchNorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, filters: int, size: int, strides: int = 2, padding: str ='same', use_bn: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.convolution = nn.ConvTranspose2d(in_channels, filters, kernel_size=size, stride=strides, padding=1)\n",
    "        self.use_bn = use_bn\n",
    "        self.batchNorm = nn.BatchNorm2d(filters)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tensor:\n",
    "        x = self.convolution(x)\n",
    "        if self.use_bn:\n",
    "            x = self.batchNorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size = LATENT_DIM) -> None:\n",
    "        super().__init__()\n",
    "        LINEAR_OUTPUT_SIZE = 4*4*256\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, LINEAR_OUTPUT_SIZE)\n",
    "        self.batchNorm = nn.BatchNorm1d(LINEAR_OUTPUT_SIZE)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.upsampleBlock1 = UpsampleBlock(in_channels=256, filters=512, size=4)\n",
    "        self.upsampleBlock2 = UpsampleBlock(in_channels=512, filters=256, size=4)\n",
    "        self.upsampleBlock3 = UpsampleBlock(in_channels=256, filters=128, size=5)\n",
    "        self.convTranspose2d = nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tensor: \n",
    "        # x - wektor szumu\n",
    "        x = self.linear(x)\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = x.view(-1, 256, 4, 4) #reshape -> channel 4x4 image\n",
    "        x = self.upsampleBlock1(x)\n",
    "        x = self.upsampleBlock2(x)\n",
    "        x = self.upsampleBlock3(x)\n",
    "        x = self.convTranspose2d(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.downsampleBlock1 = DownsampleBlock(in_channels=3, filters=64, size=4, strides=2)\n",
    "        self.downsampleBlock2 = DownsampleBlock(in_channels=64, filters=128, size=4, strides=2)\n",
    "        self.downsampleBlock3 = DownsampleBlock(in_channels=128, filters=256, size=4, strides=2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.dense = nn.Linear(256*8*8,1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tensor:\n",
    "        x = self.downsampleBlock1(x)\n",
    "        x = self.downsampleBlock2(x)\n",
    "        x = self.downsampleBlock3(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, 256*8*8)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_trainer:\n",
    "    def __init__(self, generator: nn.Module, discriminator: nn.Module, generator_optimizer: torch.optim.Optimizer,\n",
    "                 discriminator_optimizer: torch.optim.Optimizer, dataloader: torch.utils.data.DataLoader) -> None:\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "        self.loss = nn.BCELoss()\n",
    "        self.dataloader = dataloader\n",
    "        \n",
    "    def train_discriminator_step(self):\n",
    "        self.discriminator.zero_grad()\n",
    "        \n",
    "        real_images = next(iter(self.dataloader))\n",
    "        # print(real_images.shape)\n",
    "        \n",
    "        batch_size = real_images.shape[0]\n",
    "        \n",
    "        real_labels = torch.ones(batch_size, 1) - torch.rand(batch_size, 1) * 0.15\n",
    "        fake_labels = torch.randn(batch_size, 1) * 0.15\n",
    "        \n",
    "        real_labels = real_labels.clamp(0.0, 1.0)\n",
    "        fake_labels = fake_labels.clamp(0.0, 1.0)\n",
    "        \n",
    "        random_vectors = torch.randn(batch_size, LATENT_DIM)\n",
    "        fake_images = self.generator(random_vectors)\n",
    "        \n",
    "        # print(f\"Random vectors shape: {random_vectors.shape}\")\n",
    "        # print(f\"Fake images shape: {fake_images.shape}\")\n",
    "        # print(f\"Real images shape: {real_images.shape}\")\n",
    "\n",
    "        real_output = self.discriminator(real_images)\n",
    "        fake_output = self.discriminator(fake_images)\n",
    "        \n",
    "        # print(f\"Real output shape: {real_output.shape}\")\n",
    "        # print(f\"Fake output shape: {fake_output.shape}\")\n",
    "        \n",
    "        all_labels = torch.cat([real_labels, fake_labels])\n",
    "        all_output = torch.cat([real_output, fake_output])\n",
    "        \n",
    "        # print(f\"All labels shape: {all_labels.shape}\")\n",
    "        # print(f\"All output shape: {all_output.shape}\")\n",
    "        \n",
    "        loss = self.loss(all_output, all_labels)\n",
    "        loss.backward()\n",
    "        self.discriminator_optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train_generator_step(self, batch_size=16):\n",
    "        self.generator.zero_grad()\n",
    "        \n",
    "        random_vectors = torch.randn(batch_size, LATENT_DIM)\n",
    "        fake_images = self.generator(random_vectors)\n",
    "        \n",
    "        fake_output = self.discriminator(fake_images)\n",
    "        misleading_labels = torch.ones(batch_size, 1)\n",
    "        loss = self.loss(fake_output, misleading_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.generator_optimizer.step()\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch(latent_batch: torch.Tensor, generator: nn.Module):\n",
    "    generated_images = generator(latent_batch)\n",
    "    fig, axs = plt.subplots(8, 8, figsize=(16, 16))\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            axs[i, j].imshow(generated_images[i*8+j].detach().numpy().transpose(1,2,0))\n",
    "            axs[i, j].axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate and train the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "gan_trainer = GAN_trainer(generator, discriminator, torch.optim.Adam(generator.parameters(), lr=0.00015, betas=(0.5, 0.999)),\n",
    "                            torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999)), train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(EPOCHS):\n",
    "    d_loss = gan_trainer.train_discriminator_step()\n",
    "    g_loss = gan_trainer.train_generator_step()\n",
    "    print(f\"Epoch: {i}, D_loss: {d_loss}, G_loss: {g_loss}\")\n",
    "    \n",
    "    if i % 50 == 0:\n",
    "        visualize_batch(torch.randn(64, LATENT_DIM), generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
