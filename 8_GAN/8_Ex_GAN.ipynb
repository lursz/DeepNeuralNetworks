{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linux users:\n",
    "# !wget -O birds.tgz https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz?download=1\n",
    "# !tar zxvf birds.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import cv2\n",
    "# from tqdm import tqdm\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations, bboxes, dataset_path, img_size):\n",
    "        self.annotations = annotations\n",
    "        self.bboxes = bboxes\n",
    "        self.dataset_path = dataset_path\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.annotations.iloc[idx]['id']\n",
    "        image_path = self.annotations.iloc[idx]['path']\n",
    "        bbox = self.bboxes[self.bboxes['id'] == image_id].iloc[0]\n",
    "\n",
    "        img = cv2.imread(self.dataset_path + \"images/\" + image_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = img[bbox['y']:bbox['y'] + bbox['height'], bbox['x']:bbox['x'] + bbox['width'], :]\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        img = torch.tensor(img, dtype=torch.float32) / 127.5 - 1\n",
    "\n",
    "        return img\n",
    "\n",
    "dataset_path = 'CUB_200_2011/'\n",
    "IMG_SIZE = 224  # Change according to your requirements\n",
    "BATCH_SIZE = 32  # Change according to your requirements\n",
    "\n",
    "# Read txt files containing info about the bounding boxes and the file paths\n",
    "bboxes = pd.read_csv(dataset_path + 'bounding_boxes.txt', sep=\" \", names=[\"id\", \"x\", \"y\", \"width\", \"height\"]).astype(int)\n",
    "annotations = pd.read_csv(dataset_path + 'images.txt', sep=\" \", names=[\"id\", \"path\"])\n",
    "\n",
    "# Create the dataset\n",
    "custom_dataset = CustomDataset(annotations, bboxes, dataset_path, IMG_SIZE)\n",
    "\n",
    "# Define the number of samples for training and testing\n",
    "limit_training = 10000\n",
    "test = 300\n",
    "\n",
    "# Split the dataset into training and testing\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(custom_dataset, [limit_training, test])\n",
    "\n",
    "# Create DataLoader objects for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(train_loader, test_loader)#store in array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=64\n",
    "BATCH_SIZE=64\n",
    "LATENT_DIM=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling / Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DownsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, filters: int, size: int, strides: int = 2, padding: str ='same', use_bn: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.convolution = nn.Conv2d(in_channels, filters, kernel_size=size, stride=strides, padding=1)\n",
    "        self.use_bn = use_bn\n",
    "        self.batchNorm = nn.BatchNorm2d(filters)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tensor:\n",
    "        x = self.convolution(x)\n",
    "        if self.use_bn:\n",
    "            x = self.batchNorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "    \n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, filters: int, size: int, strides: int = 2, padding: str ='same', use_bn: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.convolution = nn.ConvTranspose2d(in_channels, filters, kernel_size=size, stride=strides, padding=1)\n",
    "        self.use_bn = use_bn\n",
    "        self.batchNorm = nn.BatchNorm2d(filters)\n",
    "        self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tensor:\n",
    "        x = self.convolution(x)\n",
    "        if self.use_bn:\n",
    "            x = self.batchNorm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size = LATENT_DIM) -> None:\n",
    "        super().__init__()\n",
    "        LINEAR_OUTPUT_SIZE = 4*4*256\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, LINEAR_OUTPUT_SIZE)\n",
    "        self.batchNorm = nn.BatchNorm1d(LINEAR_OUTPUT_SIZE)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.upsampleBlock1 = UpsampleBlock(in_channels=256, filters=512, size=4)\n",
    "        self.upsampleBlock2 = UpsampleBlock(in_channels=512, filters=256, size=4)\n",
    "        self.upsampleBlock3 = UpsampleBlock(in_channels=256, filters=128, size=4)\n",
    "        self.convTranspose2d = nn.ConvTranspose2d(in_channels=128, out_channels=3, kernel_size=4, stride=2, padding=2)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tensor: \n",
    "        # x - wektor szumu\n",
    "        x = self.linear(x)\n",
    "        x = self.batchNorm(x)\n",
    "        x = self.activation(x)\n",
    "        x = x.view(-1, 256, 4, 4) #reshape -> channel 4x4 image\n",
    "        x = self.upsampleBlock1(x)\n",
    "        x = self.upsampleBlock2(x)\n",
    "        x = self.upsampleBlock3(x)\n",
    "        x = self.convTranspose2d(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.downsampleBlock1 = DownsampleBlock(in_channels=3, filters=64, size=4, strides=2)\n",
    "        self.downsampleBlock2 = DownsampleBlock(in_channels=64, filters=128, size=4, strides=2)\n",
    "        self.downsampleBlock3 = DownsampleBlock(in_channels=128, filters=256, size=4, strides=2)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.dense = nn.Linear(256*8*8,1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> Tensor:\n",
    "        x = self.downsampleBlock1(x)\n",
    "        x = self.downsampleBlock2(x)\n",
    "        x = self.downsampleBlock3(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, 256*8*8)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.forward(torch.randn(2, LATENT_DIM)).shape\n",
    "discriminator.forward(torch.randn(2, 3, 64, 64)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
