{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import torchtext\n",
    "import torchtext.data\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.encoders.text import StaticTokenizerEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(train_acc_history, train_loss_history, val_acc_history, val_loss_history):\n",
    "    epochs = range(1, len(train_loss_history) + 1)\n",
    "    \n",
    "    # Loss\n",
    "    plt.figure(1)\n",
    "    plt.plot(epochs, train_loss_history, 'b', label='Training loss (' + str(format(train_loss_history[-1], '.5f')) + ')')\n",
    "    plt.plot(epochs, val_loss_history, 'g', label='Validation loss (' + str(format(val_loss_history[-1], '.5f')) + ')')\n",
    "    \n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy\n",
    "    plt.figure(2)\n",
    "    plt.plot(epochs, train_acc_history, 'b', label='Training accuracy (' + str(format(train_acc_history[-1], '.5f')) + ')')\n",
    "    plt.plot(epochs, val_acc_history, 'g', label='Validation accuracy (' + str(format(val_acc_history[-1], '.5f')) + ')')\n",
    "\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_file(path: str) -> dict[str, np.ndarray]:\n",
    "    embeddings: dict = {}\n",
    "    f = open(path, encoding =\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings))\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts() -> tuple[list[str], list[int]]:\n",
    "    imdb_dir = 'kaggle/Imdb/'\n",
    "    train_dir = imdb_dir + 'train/'\n",
    "\n",
    "    labels = []\n",
    "    texts = []\n",
    "\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = train_dir + label_type\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname), encoding =\"utf8\")\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings = read_glove_file('kaggle/glove.6B/glove.6B.100d.txt')\n",
    "texts, labels = read_texts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts: list[str], maxlen: int, max_words: int) -> tuple[torch.Tensor, dict[any, int]]:\n",
    "    tokenizer = torchtext.data.utils.get_tokenizer(None)    \n",
    "    filter_tokens = lambda x: [word for word in tokenizer(x) if word in embeddings]\n",
    "    text_sequences = [filter_tokens(text) for text in texts]\n",
    "    all_tokens_set = set()\n",
    "    for seq in text_sequences:\n",
    "        for token in seq:\n",
    "            all_tokens_set.add(token)\n",
    "    \n",
    "    all_embedding_tokens = set(embeddings.keys())\n",
    "    tokens = all_tokens_set.intersection(all_embedding_tokens)\n",
    "\n",
    "    word_index = {word: i + 1 for i, word in enumerate(tokens) if i + 1 < max_words}\n",
    "    sequences = [[word_index[word] for word in text if word in word_index] for text in text_sequences]\n",
    "    padded_sequences = torch.zeros(len(sequences), maxlen, dtype=torch.long)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            seq = seq[:maxlen]\n",
    "        else:\n",
    "            seq = seq + [0] * (maxlen - len(seq))\n",
    "        padded_sequences[i] = torch.tensor(seq)\n",
    "\n",
    "    return padded_sequences, word_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(max_words: int, embedding_index: dict[str, np.ndarray], word_index: dict[any, int]) -> np.ndarray:\n",
    "    embedding_dimension = embedding_index.get('the').shape[0]\n",
    "\n",
    "    embedding_matrix = np.zeros((max_words, embedding_dimension))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_words:\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # Words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data: np.ndarray, labels: list[int], train_part: float, validation_part: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    data_size = data.shape[0]\n",
    "    train_size = int(data_size * train_part)\n",
    "    validation_size = int(data_size * validation_part)\n",
    "    test_size = data_size - train_size - validation_size\n",
    "\n",
    "    indices = np.arange(data_size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    validation_indices = indices[train_size:train_size + validation_size]\n",
    "    test_indices = indices[train_size + validation_size:]\n",
    "\n",
    "    train_data = data[train_indices]\n",
    "    validation_data = data[validation_indices]\n",
    "    test_data = data[test_indices]\n",
    "\n",
    "    train_labels = labels[train_indices]\n",
    "    validation_labels = labels[validation_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "\n",
    "    print(f\"Train data size: {train_data.shape[0]}\")\n",
    "    print(f\"Validation data size: {validation_data.shape[0]}\")\n",
    "    print(f\"Test data size: {test_data.shape[0]}\")\n",
    "\n",
    "    return train_data, validation_data, test_data, train_labels, validation_labels, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(maxlen: int, max_words: int, embeddings: dict[str, np.ndarray], texts: list[str], labels: list[int], train_part: float, validation_part: float) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, dict[any, int], np.ndarray]:\n",
    "    data, word_index = texts_to_sequences(texts, maxlen=maxlen, max_words=max_words)\n",
    "    embedding_matrix = get_embedding_matrix(max_words, embeddings, word_index)\n",
    "    train_data, validation_data, test_data, train_labels, validation_labels, test_labels = split_data(data, labels, train_part, validation_part)\n",
    "    return train_data, validation_data, test_data, train_labels, validation_labels, test_labels, word_index, embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 20000\n",
      "Validation data size: 2500\n",
      "Test data size: 2500\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data, train_labels, validation_labels, test_labels, word_index, embedding_matrix = prepare_data_for_model(maxlen=100, max_words=10000, embeddings=embeddings, texts=texts, labels=labels, train_part=0.8, validation_part=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(train_data, torch.tensor(train_labels, dtype=torch.long))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "validation_dataset = torch.utils.data.TensorDataset(validation_data, torch.tensor(validation_labels, dtype=torch.long))\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data, torch.tensor(test_labels, dtype=torch.long))\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS: int = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embed_size: int, maxlen: int):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.conv = nn.Conv1d(embed_size, 32, kernel_size=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        conv_output_size = (maxlen - 2) // 2\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(32 * conv_output_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def set_embedding(self, embedding_matrix: np.ndarray) -> None:\n",
    "        assert self.embedding.weight.shape == embedding_matrix.shape, \"Embedding matrix shape mismatch\"\n",
    "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        print(\"Embedding shape:\", self.embedding.weight.data.shape)\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)  # Change shape to [batch_size, embed_size, seq_len] for Conv1d\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: nn.Module, criterion: nn.Module, optimizer: optim.Optimizer,\n",
    "                train_loader: torch.utils.data.DataLoader, validation_loader: torch.utils.data.DataLoader) -> tuple[list[float], list[float], list[float], list[float]]:\n",
    "    model.to(device)\n",
    "    criterion.to(device)\n",
    "    # torch.cuda.empty_cache()\n",
    "    accuracy_history: list = []\n",
    "    loss_history: list = []\n",
    "    val_accuracy_history: list = []\n",
    "    val_loss_history: list = []\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, target in tqdm(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target.float().view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            predicted = (output > 0.5).float()\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target.float().view(-1, 1)).sum().item()\n",
    "        accuracy = correct / total\n",
    "        loss_history.append(train_loss / len(train_loader))\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in validation_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target.float().view(-1, 1))\n",
    "                val_loss += loss.item()\n",
    "                predicted = (output > 0.5).float()\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target.float().view(-1, 1)).sum().item()\n",
    "        val_accuracy = correct / total\n",
    "        val_loss_history.append(val_loss / len(validation_loader))\n",
    "        val_accuracy_history.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{EPOCHS}:\")\n",
    "        print(f\"Train loss: {loss_history[-1]}, accuracy: {accuracy}\")\n",
    "        print(f\"Validation loss: {val_loss_history[-1]}, accuracy: {val_accuracy}\")\n",
    "        \n",
    "    return accuracy_history, loss_history, val_accuracy_history, val_loss_history\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([10000, 100])\n",
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/625 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 113.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train loss: 0.6524694971084595, accuracy: 0.61135\n",
      "Validation loss: 0.6140984964521625, accuracy: 0.6776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:04<00:00, 125.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "Train loss: 0.6028225239276886, accuracy: 0.6706\n",
      "Validation loss: 0.6044006743763066, accuracy: 0.676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 121.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "Train loss: 0.5810943242549896, accuracy: 0.6943\n",
      "Validation loss: 0.5836772235888469, accuracy: 0.7032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:04<00:00, 125.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "Train loss: 0.5606883839607238, accuracy: 0.71165\n",
      "Validation loss: 0.5881670558754402, accuracy: 0.6912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:06<00:00, 98.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "Train loss: 0.5441222069740296, accuracy: 0.7231\n",
      "Validation loss: 0.5753334353241739, accuracy: 0.7008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 121.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "Train loss: 0.5265481192588806, accuracy: 0.7386\n",
      "Validation loss: 0.577605883154688, accuracy: 0.706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 119.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "Train loss: 0.5095807530403137, accuracy: 0.74805\n",
      "Validation loss: 0.614638537922992, accuracy: 0.6832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 118.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "Train loss: 0.4962110005378723, accuracy: 0.75705\n",
      "Validation loss: 0.5967873636680313, accuracy: 0.6996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 119.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "Train loss: 0.4837517150878906, accuracy: 0.76385\n",
      "Validation loss: 0.5960542819922483, accuracy: 0.6928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 117.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "Train loss: 0.4730104826450348, accuracy: 0.7699\n",
      "Validation loss: 0.6033857453473007, accuracy: 0.6948\n"
     ]
    }
   ],
   "source": [
    "model = TextClassificationModel(vocab_size=10000, embed_size=100, maxlen=100)\n",
    "model.set_embedding(embedding_matrix)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "accuracy_history, loss_history, val_accuracy_history, val_loss_history = train_model(model, criterion, optimizer, train_loader, validation_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
