{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.encoders.text import StaticTokenizerEncoder\n",
    "from torchnlp.encoders import pad_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_file(path: str = 'glove') -> dict[str, np.ndarray]:\n",
    "    embeddings_index = {}\n",
    "    f = open(path, encoding =\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "\n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts() -> tuple[list[str], list[int]]:\n",
    "    imdb_dir = 'data/aclImdb'\n",
    "    train_dir = imdb_dir + 'train'\n",
    "\n",
    "    labels = []\n",
    "    texts = []\n",
    "\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = train_dir + label_type\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname[-4:] == '.txt':\n",
    "                f = open(os.path.join(dir_name, fname), encoding =\"utf8\")\n",
    "                texts.append(f.read())\n",
    "                f.close()\n",
    "                if label_type == 'neg':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts_to_sequences(texts: list[str], maxlen: int, max_words: int) -> tuple[torch.Tensor, dict[any, int]]:\n",
    "    tokenizer = StaticTokenizerEncoder(texts, min_occurrences=1, reserved_tokens=['<pad>'], num_tokens=max_words)\n",
    "\n",
    "    # Tokenize the texts\n",
    "    sequences = [tokenizer.encode(text) for text in texts]\n",
    "    # Create a word index (token to index mapping)\n",
    "    word_index = tokenizer.token_to_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    # Pad the sequences\n",
    "    padded_sequences = [pad_tensor(sequence, max_length=maxlen, padding_index=tokenizer.token_to_index['<pad>']) for sequence in sequences]\n",
    "    # Convert to a tensor\n",
    "    data = torch.stack(padded_sequences)\n",
    "\n",
    "    return data, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(max_words: int, embedding_index: dict[str, np.ndarray], word_index: dict[Any, int]) -> np.ndarray:\n",
    "    embedding_dimension = embedding_index.get('the').shape[0]\n",
    "\n",
    "    embedding_matrix = np.zeros((max_words, embedding_dimension))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_words:\n",
    "            embedding_vector = embedding_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # Words not found in embedding index will be all-zeros.\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data: np.ndarray, labels: List[int], train_part: float, validation_part: float) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    data_size = data.shape[0]\n",
    "    train_size = int(data_size * train_part)\n",
    "    validation_size = int(data_size * validation_part)\n",
    "    test_size = data_size - train_size - validation_size\n",
    "\n",
    "    indices = np.arange(data_size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    validation_indices = indices[train_size:train_size + validation_size]\n",
    "    test_indices = indices[train_size + validation_size:]\n",
    "\n",
    "    train_data = data[train_indices]\n",
    "    validation_data = data[validation_indices]\n",
    "    test_data = data[test_indices]\n",
    "\n",
    "    train_labels = labels[train_indices]\n",
    "    validation_labels = labels[validation_indices]\n",
    "    test_labels = labels[test_indices]\n",
    "\n",
    "    print(f\"Train data size: {train_data.shape[0]}\")\n",
    "    print(f\"Validation data size: {validation_data.shape[0]}\")\n",
    "    print(f\"Test data size: {test_data.shape[0]}\")\n",
    "\n",
    "    return train_data, validation_data, test_data, train_labels, validation_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
